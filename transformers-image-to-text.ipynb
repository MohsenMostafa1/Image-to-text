{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9095327,"sourceType":"datasetVersion","datasetId":5488877}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformers Image-to-text\n\n### Image-to-text conversion is a transformative technology that enables the extraction of textual information from images, making it a vital tool in various applications. This process typically employs Optical Character Recognition (OCR) and image captioning techniques to facilitate the transition from visual data to machine-readable text.\n\n<figure>\n        <img src=\"https://buffer.com/library/content/images/size/w1200/2023/10/free-images.jpg\" alt =\"Audio Art\" style='width:800px;height:500px;'>\n        <figcaption>","metadata":{}},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport requests\n\n# Load the processor and model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T10:30:51.676113Z","iopub.execute_input":"2024-08-03T10:30:51.676577Z","iopub.status.idle":"2024-08-03T10:31:24.467514Z","shell.execute_reply.started":"2024-08-03T10:30:51.676531Z","shell.execute_reply":"2024-08-03T10:31:24.466177Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-03 10:31:00.484822: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-03 10:31:00.484956: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-03 10:31:00.627231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca230cf509264582b8bb5b9864246929"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36c611932ce4996a4052b0801c35661"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"608f823f55b049d08941872cf0a77271"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fcb31d91d1343b49da3ea23a0e4175f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c23656b456c448cbbd9e44004ef972d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f174637ad7364d30b36cfa472738753c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6bb269822474ecf831091a6eaf95520"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"image = Image.open(\"/kaggle/input/free-images/free-images.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T10:35:39.481493Z","iopub.execute_input":"2024-08-03T10:35:39.481937Z","iopub.status.idle":"2024-08-03T10:35:39.497536Z","shell.execute_reply.started":"2024-08-03T10:35:39.481901Z","shell.execute_reply":"2024-08-03T10:35:39.496502Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Preprocess the image and generate the caption\ninputs = processor(image, return_tensors=\"pt\")\noutputs = model.generate(**inputs)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T10:35:41.078241Z","iopub.execute_input":"2024-08-03T10:35:41.078673Z","iopub.status.idle":"2024-08-03T10:35:44.108585Z","shell.execute_reply.started":"2024-08-03T10:35:41.078639Z","shell.execute_reply":"2024-08-03T10:35:44.107374Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Decode and print the generated caption\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated caption:\", caption)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T10:35:47.229684Z","iopub.execute_input":"2024-08-03T10:35:47.230459Z","iopub.status.idle":"2024-08-03T10:35:47.237482Z","shell.execute_reply.started":"2024-08-03T10:35:47.230414Z","shell.execute_reply":"2024-08-03T10:35:47.236096Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Generated caption: a woman taking a photo with a camera\n","output_type":"stream"}]}]}